# FDA-3-Stream: Complete Project Guide (A to Z)

## ðŸ“š Table of Contents
1. [Project Overview](#project-overview)
2. [What Problem Are We Solving?](#what-problem-are-we-solving)
3. [System Architecture](#system-architecture)
4. [Dataset Description](#dataset-description)
5. [Code Components Explained](#code-components-explained)
6. [How Everything Works Together](#how-everything-works-together)
7. [Key Concepts Explained](#key-concepts-explained)
8. [Presentation Guide](#presentation-guide)
9. [FAQ - Common Questions](#faq---common-questions)

---

## ðŸŽ¯ Project Overview

### What is FDA-3-Stream?

**FDA-3-Stream** is a real-time streaming analytics system that monitors and analyzes revenue recognition practices from SEC 10-K filings. It detects potentially aggressive accounting practices or inconsistencies in how companies recognize revenue.

### The Name Breakdown:
- **FDA-3**: Financial Data Analytics - 3 (likely referring to the third iteration or module)
- **Stream**: Real-time streaming data processing

### Core Technology Stack:
- **Apache Kafka**: Message streaming platform
- **River (Python)**: Online/incremental machine learning library
- **CapyMOA-style**: Drift detection algorithms (DDM, EDDM)
- **Streamlit**: Interactive dashboard for visualization

---

## ðŸ” What Problem Are We Solving?

### The Business Problem:

Companies must follow accounting standards (like GAAP) when reporting revenue. However, some companies may:
1. **Recognize revenue too early** (aggressive accounting)
2. **Use inconsistent methods** across periods
3. **Change recognition policies** without proper disclosure

### Why This Matters:

- **Investors** need accurate financial information
- **Regulators** (SEC) monitor for compliance
- **Auditors** need to detect anomalies
- **Analysts** want to identify red flags

### Our Solution:

We built a **real-time monitoring system** that:
- Continuously analyzes revenue recognition disclosures
- Detects patterns and anomalies automatically
- Flags potential issues in real-time
- Provides explainable insights

---

## ðŸ—ï¸ System Architecture

### High-Level Flow:

```
SEC 10-K Filings (Data Source)
         â†“
    CSV Dataset
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Producer        â”‚  â† Module 1: Feature Generation
â”‚  (kafka_producer.py)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
    â”‚               â”‚
    â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text   â”‚   â”‚  Numeric    â”‚
â”‚  Stream â”‚   â”‚  Stream     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
     â”‚               â”‚
     â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  River Consumers            â”‚  â† Module 2: Text Classification
â”‚  (kafka_consumer_river_     â”‚     Module 3: Regression & Drift
â”‚   enhanced.py)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CapyMOA Drift Detector     â”‚  â† Module 3: Drift Detection
â”‚  (capy_drift.py)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Fusion Consumer            â”‚  â† Module 4: Cross-Platform Fusion
â”‚  (in enhanced consumer)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Alert Topic                â”‚
â”‚  (revenue_alerts)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard                  â”‚  â† Module 6: Visualization
â”‚  (dashboard_app.py)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components Breakdown:

1. **Kafka Topics** (3 topics):
   - `revenue_text_stream`: Text data (policy descriptions)
   - `revenue_numeric_stream`: Numeric data (ratios, revenue amounts)
   - `revenue_alerts`: All alerts generated by the system

2. **Processing Components**:
   - Producer: Generates and streams data
   - Consumers: Process and analyze data
   - Drift Detector: Monitors for changes
   - Fusion: Combines multiple signals
   - Dashboard: Visualizes results

---

## ðŸ“Š Dataset Description

### File: `datasets/revenue_patterns_sample.csv`

### What's in the Dataset?

The dataset contains revenue recognition information from SEC 10-K filings for various companies.

### Columns Explained:

1. **`company`** (String)
   - Full company name (e.g., "Apple Inc.")
   - **Why**: Identifies which company the data belongs to

2. **`ticker`** (String)
   - Stock ticker symbol (e.g., "AAPL", "MSFT")
   - **Why**: Short identifier for the company

3. **`period`** (String)
   - Financial reporting period (e.g., "2025Q3")
   - **Why**: Shows which quarter/year the data is from

4. **`policy_text`** (String)
   - The actual revenue recognition policy text from 10-K filing
   - **Why**: Contains the language we analyze to classify as Aggressive/Conservative/Neutral
   - **Example**: "Apple Inc. uses: Reported revenue ~391,035,000,000"

5. **`revenue`** (Float)
   - Total revenue in dollars (e.g., 391035000000.0)
   - **Why**: The actual revenue amount we're analyzing

6. **`revenue_cashflow`** (Float)
   - Cash flow from operations related to revenue
   - **Why**: Used to calculate Revenue-to-CashFlow ratio (indicates if revenue is backed by cash)

7. **`deferred_revenue`** (Float)
   - Revenue that's been received but not yet recognized
   - **Why**: High deferred revenue might indicate conservative accounting

8. **`rev_to_cash`** (Float, Computed)
   - Ratio: Revenue / Cash Flow
   - **Why**: 
     - High ratio (>5) might indicate aggressive recognition
     - Low ratio (<2) might indicate conservative recognition
   - **Formula**: `revenue / (revenue_cashflow + 1e-9)`

9. **`deferred_ratio`** (Float, Computed)
   - Ratio: Deferred Revenue / Total Revenue
   - **Why**: 
     - High ratio indicates conservative accounting (holding revenue)
     - Low ratio might indicate aggressive recognition
   - **Formula**: `deferred_revenue / (revenue + 1e-9)`

### Dataset Statistics:

- **Size**: ~328 rows (companies/periods)
- **Companies**: Various S&P 500 companies
- **Time Period**: Q3 2025 data
- **Format**: CSV (Comma-Separated Values)

### What Makes This Data Interesting?

1. **Real Financial Data**: Actual revenue recognition practices
2. **Variety**: Different companies use different methods
3. **Patterns**: Some companies consistently use aggressive/conservative methods
4. **Anomalies**: Some periods show unusual patterns (drift)

---

## ðŸ’» Code Components Explained

### 1. `scripts/kafka_producer.py` - Module 1

**What it does:**
- Reads the CSV dataset
- Extracts text and numeric features
- Streams data to Kafka topics

**Why it's needed:**
- Simulates real-time data ingestion (like from SEC EDGAR)
- Separates text and numeric data for different processing

**Key Functions:**

```python
def make_producer(bootstrap):
    # Creates Kafka producer to send messages
    
def iso_now():
    # Gets current timestamp in ISO format
    
def main():
    # Main loop that:
    # 1. Reads CSV row by row
    # 2. Creates text message (company, ticker, policy_text)
    # 3. Creates numeric message (revenue, ratios)
    # 4. Sends both to Kafka
    # 5. Waits (interval) before next row
```

**What it sends:**

**Text Topic Message:**
```json
{
  "company": "Apple Inc.",
  "ticker": "AAPL",
  "period": "2025Q3",
  "policy_text": "Apple Inc. uses: Reported revenue...",
  "ts": "2025-01-15T10:30:00"
}
```

**Numeric Topic Message:**
```json
{
  "company": "Apple Inc.",
  "ticker": "AAPL",
  "period": "2025Q3",
  "revenue": 391035000000.0,
  "cashflow": 118254000000.0,
  "deferred_revenue": 0.0,
  "rev_to_cash": 3.3067,
  "deferred_ratio": 0.0,
  "ts": "2025-01-15T10:30:00"
}
```

---

### 2. `scripts/kafka_consumer_river_enhanced.py` - Modules 2, 3, 4, 5

**What it does:**
- Consumes text and numeric streams
- Classifies text using ML models
- Predicts revenue using regression
- Detects drift in predictions
- Fuses multiple signals
- Tracks feature importance

**Why it's needed:**
- Processes data in real-time
- Learns incrementally (online learning)
- Adapts to changes (drift detection)

#### Module 2: Text Classification

**Models Used:**

1. **MultinomialNB (Naive Bayes)**
   - **What**: Probabilistic classifier
   - **Why**: Fast, works well with text
   - **How**: Learns word probabilities for each class

2. **LogisticRegression**
   - **What**: Linear classifier (binary, used in OneVsRest)
   - **Why**: Provides probability scores
   - **How**: Uses two binary classifiers (Aggressive vs Not, Conservative vs Not)

3. **HoeffdingTreeClassifier**
   - **What**: Decision tree for streaming data
   - **Why**: Handles concept drift well
   - **How**: Builds tree incrementally

**Classification Labels:**

- **Aggressive**: Contains words like "variable consideration", "multiple-element", "upfront"
- **Conservative**: Contains words like "immaterial", "consistent", "prudent"
- **Neutral**: Everything else

**Metrics Tracked:**
- **Accuracy**: % of correct predictions
- **F1-Score**: Balance between precision and recall

#### Module 3: Regression & Drift Detection

**Regression Model:**

- **AdaptiveRandomForestRegressor**
  - **What**: Ensemble of decision trees
  - **Why**: Adapts to changing patterns
  - **Input**: `rev_to_cash`, `deferred_ratio`
  - **Output**: Predicted revenue
  - **Metrics**: MAE, RMSE, RÂ²

**Drift Detection:**

- **ADWIN (Adaptive Windowing)**
  - **What**: Detects changes in data distribution
  - **Why**: Flags when revenue patterns change
  - **How**: Monitors prediction residuals

**Metrics Tracked:**
- **MAE**: Mean Absolute Error (average prediction error)
- **RMSE**: Root Mean Squared Error (penalizes large errors)
- **RÂ²**: Coefficient of determination (how well model fits)

#### Module 4: Fusion

**What it does:**
- Monitors all alerts
- Combines signals from multiple detectors
- Flags "High-Risk" when multiple indicators agree

**High-Risk Conditions:**
- Aggressive text classification AND
- Numeric drift detected OR
- CapyMOA drift detected

**Consensus Rate:**
- Calculates agreement between detectors
- Higher consensus = more reliable alert

#### Module 5: Explainability

**Feature Importance Tracking:**

- **Text Features**: Tracks which words are most important
- **Numeric Features**: Tracks which ratios matter most
- **Why**: Helps understand why alerts were triggered

---

### 3. `scripts/capy_drift.py` - Module 3 (CapyMOA)

**What it does:**
- Monitors numeric stream for drift
- Uses multiple drift detectors
- Publishes alerts when drift detected

**Drift Detectors:**

1. **ADWIN** (Adaptive Windowing)
   - Detects distribution changes
   - Adaptive window size

2. **DDM** (Drift Detection Method)
   - Binary drift detector
   - Works on error rates

3. **EDDM** (Early Drift Detection Method)
   - Improved version of DDM
   - Better for gradual drift

4. **KSWIN** (Kolmogorov-Smirnov Windowing)
   - Statistical test for distribution changes

5. **PageHinkley**
   - Detects mean shifts
   - Good for abrupt changes

**Drift Score Calculation:**

```python
score = rev_to_cash * 10.0 + deferred_ratio * 100.0
```

**Why this formula:**
- Combines two key indicators
- Scaled to make drift more detectable
- Higher score = more unusual pattern

---

### 4. `scripts/dashboard_app.py` - Module 6

**What it does:**
- Multi-page Streamlit application
- Visualizes alerts and metrics
- Provides project documentation

**Pages:**

1. **Project Overview**
   - Complete project documentation
   - Module descriptions
   - Technical details

2. **Real-time Dashboard**
   - Live alert visualization
   - KPIs and metrics
   - Interactive filters

**Key Features:**
- Real-time updates (optional auto-refresh)
- Interactive filters
- Export functionality
- Multiple visualizations

---

## ðŸ”„ How Everything Works Together

### Step-by-Step Flow:

1. **Data Ingestion** (Producer)
   ```
   CSV â†’ Read row â†’ Extract features â†’ Send to Kafka
   ```

2. **Text Processing** (Consumer)
   ```
   Kafka Text Topic â†’ Classify with 3 models â†’ Ensemble prediction â†’ Send alert
   ```

3. **Numeric Processing** (Consumer)
   ```
   Kafka Numeric Topic â†’ Predict revenue â†’ Calculate residual â†’ Detect drift â†’ Send alert
   ```

4. **Drift Detection** (CapyMOA)
   ```
   Kafka Numeric Topic â†’ Calculate drift score â†’ Update detectors â†’ Detect drift â†’ Send alert
   ```

5. **Fusion** (Consumer)
   ```
   All Alerts â†’ Check for consensus â†’ Flag high-risk â†’ Send alert
   ```

6. **Visualization** (Dashboard)
   ```
   All Alerts â†’ Display KPIs â†’ Show charts â†’ Filter/search
   ```

### Real-Time Processing:

- **Producer**: Sends 1 message per second (configurable)
- **Consumers**: Process messages as they arrive
- **Dashboard**: Updates every 5 seconds (if auto-refresh enabled)

---

## ðŸ“– Key Concepts Explained

### 1. What is Kafka?

**Kafka** is a distributed streaming platform.

**Think of it like:**
- A **message queue** where producers send messages
- Consumers read messages from the queue
- Messages are organized into **topics**

**Why we use it:**
- Handles high throughput
- Decouples producers and consumers
- Stores messages temporarily
- Multiple consumers can read same messages

### 2. What is Online/Incremental Learning?

**Traditional ML:**
- Train on entire dataset
- Model is fixed
- Retrain from scratch for new data

**Online Learning (River):**
- Learn one example at a time
- Model updates continuously
- Adapts to new patterns automatically

**Why it matters:**
- Real-time adaptation
- No need to retrain
- Handles streaming data

### 3. What is Concept Drift?

**Concept Drift** = When the underlying data distribution changes over time.

**Example:**
- Company changes revenue recognition method
- Economic conditions change
- New accounting standards

**Why detect it:**
- Model performance degrades
- Need to adapt or retrain
- Flag potential issues

### 4. What is Feature Importance?

**Feature Importance** = Which features (words, ratios) matter most for predictions.

**Why it matters:**
- Understand model decisions
- Identify key indicators
- Explainable AI

### 5. What are the Ratios?

**Revenue-to-CashFlow Ratio:**
- **High (>5)**: Revenue recognized but cash not received yet â†’ Aggressive
- **Low (<2)**: Revenue matches cash flow â†’ Conservative

**Deferred Revenue Ratio:**
- **High**: Company holds revenue â†’ Conservative
- **Low**: Revenue recognized immediately â†’ Aggressive

---

## ðŸŽ¤ Presentation Guide

### Opening (30 seconds):

"Good [morning/afternoon]. Today I'll present FDA-3-Stream, a real-time system that monitors revenue recognition practices from SEC filings to detect aggressive accounting or inconsistencies."

### Problem Statement (1 minute):

"Companies must follow accounting standards when reporting revenue. However, some may recognize revenue too early or use inconsistent methods. Our system automatically monitors and flags these issues in real-time."

### Solution Overview (1 minute):

"We built a streaming analytics system using Kafka, River ML, and CapyMOA that:
- Analyzes revenue recognition text and financial ratios
- Uses machine learning to classify practices
- Detects drift and anomalies
- Provides real-time alerts through a dashboard"

### Technical Deep Dive (3-4 minutes):

**Module 1 - Data Ingestion:**
"Our producer reads SEC filing data and streams it to Kafka topics, separating text and numeric data."

**Module 2 - Text Classification:**
"We use three ML models - Naive Bayes, Logistic Regression, and Hoeffding Tree - to classify revenue recognition policies as Aggressive, Conservative, or Neutral."

**Module 3 - Regression & Drift:**
"We predict revenue using Adaptive Random Forest and monitor for drift using five different detectors including ADWIN, DDM, and EDDM."

**Module 4 - Fusion:**
"We combine signals from multiple detectors to identify high-risk cases with consensus."

**Module 5 - Explainability:**
"We track feature importance to understand which words and ratios drive our predictions."

**Module 6 - Dashboard:**
"Our interactive dashboard visualizes alerts, KPIs, and metrics in real-time."

### Demo (2-3 minutes):

1. Show producer sending data
2. Show consumers processing
3. Show alerts appearing in dashboard
4. Explain KPIs and visualizations

### Results & Metrics (1 minute):

"We track:
- Text classification accuracy and F1-score
- Regression MAE, RMSE, and RÂ²
- Drift detection frequency
- Cross-model consensus rates"

### Conclusion (30 seconds):

"This system provides real-time monitoring of revenue recognition practices, helping investors, regulators, and auditors identify potential issues automatically."

---

## â“ FAQ - Common Questions

### Q1: Why use Kafka instead of a database?

**A:** Kafka is designed for streaming data:
- Handles high throughput
- Multiple consumers can read same data
- Decouples producers and consumers
- Better for real-time processing

### Q2: Why three text classification models?

**A:** Ensemble approach:
- Different models catch different patterns
- Majority vote is more reliable
- Reduces overfitting
- Better accuracy

### Q3: What's the difference between River and CapyMOA?

**A:** 
- **River**: General online ML library (classification, regression)
- **CapyMOA**: Specialized drift detection algorithms
- We use both for comprehensive monitoring

### Q4: How do you know if drift is real or noise?

**A:** 
- Multiple detectors must agree (consensus)
- We track detection frequency
- High-risk alerts require multiple signals
- Historical patterns help validate

### Q5: What if the model makes wrong predictions?

**A:**
- Models learn incrementally and adapt
- We track accuracy metrics
- Ensemble reduces individual model errors
- Feature importance helps debug

### Q6: How scalable is this system?

**A:**
- Kafka handles millions of messages
- Consumers can be scaled horizontally
- Models process one message at a time (efficient)
- Dashboard can handle thousands of alerts

### Q7: What's the latency?

**A:**
- Producer â†’ Consumer: < 1 second
- Consumer â†’ Alert: < 1 second
- Total: ~2 seconds end-to-end

### Q8: How do you handle missing data?

**A:**
- Default values (0.0 for missing numbers)
- Empty strings for missing text
- Models handle missing features gracefully
- Validation in producer

### Q9: Can this detect fraud?

**A:**
- Detects anomalies and inconsistencies
- Flags potential issues
- Requires human review for confirmation
- Provides evidence (feature importance)

### Q10: What's next for this project?

**A:**
- Connect to real SEC EDGAR API
- Add more ML models
- Improve drift detection sensitivity
- Add more visualizations
- Deploy to production

---

## ðŸŽ“ Key Takeaways

1. **Real-time Processing**: System processes data as it streams
2. **Multiple Models**: Ensemble approach for better accuracy
3. **Drift Detection**: Adapts to changing patterns
4. **Explainable**: Feature importance shows why alerts trigger
5. **Scalable**: Built with streaming architecture
6. **Complete Pipeline**: End-to-end from data to dashboard

---

## ðŸ“ Quick Reference

### Run Commands:
```bash
# Terminal 1: Drift Detector
python3 scripts/capy_drift.py

# Terminal 2: Enhanced Consumers
python3 scripts/kafka_consumer_river_enhanced.py

# Terminal 3: Producer
python3 scripts/kafka_producer.py --csv datasets/revenue_patterns_sample.csv --interval 1.0

# Terminal 4: Dashboard
streamlit run scripts/dashboard_app.py
```

### Key Files:
- `kafka_producer.py`: Data ingestion
- `kafka_consumer_river_enhanced.py`: Main processing
- `capy_drift.py`: Drift detection
- `dashboard_app.py`: Visualization
- `revenue_patterns_sample.csv`: Dataset

### Key Metrics:
- **Accuracy**: Text classification correctness
- **F1-Score**: Balanced classification metric
- **MAE/RMSE**: Regression error metrics
- **RÂ²**: Model fit quality
- **Consensus Rate**: Detector agreement

---

**This guide covers everything you need to know about the FDA-3-Stream project. Study this thoroughly to present confidently and answer any questions!**

